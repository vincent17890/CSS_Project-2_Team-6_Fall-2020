{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Social Science Project #2 \n",
    "Group members: Pei-Ming (Vincent) Chen, Lawrence Liu, & Kelly Quinn \n",
    "\n",
    "Date: 10/20/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import necessary packages\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, scale\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import plotly.graph_objects as go\n",
    "import nbformat\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, r2_score\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# use random seed for consistent results \n",
    "import random\n",
    "random.seed(273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Cleaning for Exploratory Data Analysis\n",
    "\n",
    "diabetes_filename = 'Diabetes with Population Info by County 2017.csv'\n",
    "df = pd.read_csv(diabetes_filename, dtype={\"CountyFIPS\": str}) #CountyFips needs to be a string so leading 0 isn't dropped\n",
    "\n",
    "df = df[df.Diabetes_Number != \"Suppressed\"] #drop row with \"Suppressed\" value in Diabetes_Number\n",
    "df[\"Diabetes_Number\"] = df[\"Diabetes_Number\"].astype('int64') #convert Diabetes Number to int\n",
    "df['Rate_Per_Thousand'] = df['Diabetes_Number']*1000\n",
    "\n",
    "#df[\"Diabetes_Number_PC\"] = df[\"Diabetes_Number\"] / df[\"race_total population\"] #create diabetes number per capita column\n",
    "\n",
    "df = df[df[\"Obesity_Number\"] != 'No Data']\n",
    "df = df[df['Physical_Inactivity_Number'] != \"No Data\"]\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "target_col = 'sex and age_total population_65 years and over_sex ratio (males per 100 females)'\n",
    "df = df[df[target_col] != '-']\n",
    "\n",
    "##Remove duplicate columns\n",
    "\n",
    "## Identify more duplicate columns - source: https://thispointer.com/how-to-find-drop-duplicate-columns-in-a-dataframe-python-pandas/ - kq\n",
    "def getDuplicateColumns(df):\n",
    "    '''\n",
    "    Get a list of duplicate columns.\n",
    "    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n",
    "    :param df: Dataframe object\n",
    "    :return: List of columns whose contents are duplicates.\n",
    "    '''\n",
    "    duplicateColumnNames = set()\n",
    "    # Iterate over all the columns in dataframe\n",
    "    for x in range(df.shape[1]):\n",
    "        # Select column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            # Select column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    "            # Check if two columns at x 7 y index are equal\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    "    return list(duplicateColumnNames)\n",
    "\n",
    "duplicateColumnNames = getDuplicateColumns(df)\n",
    "print('Duplicate Columns are as follows')\n",
    "for col in duplicateColumnNames:\n",
    "    print('Column name : ', col)\n",
    "\n",
    "#drop all duplicated columns\n",
    "df = df.drop(columns=['race_total population_one race_1', 'race_total population_two or more races_1',\n",
    "                     'sex and age_total population_18 years and over_1', \n",
    "                      'sex and age_total population_65 years and over_1',\n",
    "                      'sex and age_total population', \n",
    "                      'race alone or in combination with one or more other races_total population',\n",
    "                      'hispanic or latino and race_total population'\n",
    "                     ]) \n",
    "\n",
    "#drop ratio + median columns\n",
    "df = df.drop(columns = ['sex and age_total population_65 years and over_sex ratio (males per 100 females)', \n",
    "                       'sex and age_total population_18 years and over_sex ratio (males per 100 females)', \n",
    "                       'sex and age_total population_median age (years)']) \n",
    "\n",
    "df.shape # 3112*86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we convert existing counts to percentages to calculate rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select count variables to rc to percentages\n",
    "rc_cols = [col for col in df.columns if col not in ['County', 'State', 'CountyFIPS']]\n",
    "           \n",
    "           \n",
    "df[rc_cols] = df[rc_cols].apply(pd.to_numeric, errors='coerce') # recode to numeric\n",
    "\n",
    "# divide all columns but those listed above by total population \n",
    "df[rc_cols] = df[rc_cols].div(df['race_total population'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check work: all rates are bounded by 0 and 1 as expected\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_summary = df.describe().transpose()\n",
    "\n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', None): \n",
    "    display(df_summary.iloc[ : ,[0,1,3,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Choropleth Map: Diabates Number by County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation relied upon at: https://plotly.com/python/choropleth-maps/\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "    \n",
    "from plotly import express as px\n",
    "\n",
    "df_choro = df[['CountyFIPS', 'Diabetes_Number']] #make dataframe with two columns to speed up process\n",
    "\n",
    "fig = px.choropleth(df_choro, geojson=counties, locations='CountyFIPS', color='Diabetes_Number',\n",
    "                           color_continuous_scale=\"Viridis\", \n",
    "                           scope=\"usa\",\n",
    "                           labels={'Diabetes_Number':'Diabetes Rate'}\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choropleth map illustrates how diabetes rates are distributed across the country. We notice that  diabetes rates in some counties are above 20% of the county population. We also see that the counties with the highest diabetes rates tend to be concentrated in the Southeastern United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii) Diabetes Rates by Subgroup Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iia) Diabetes Rate by County Age Breakdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc('font', weight='bold')\n",
    "\n",
    "bar_u25 = df['sex and age_total population_under 5 years']*100 + df['sex and age_total population_5 to 9 years']*100 + df['sex and age_total population_10 to 14 years']*100 + df['sex and age_total population_15 to 19 years'] * 100 + df['sex and age_total population_20 to 24 years']*100 \n",
    "bar_2544 = df['sex and age_total population_25 to 34 years']*100 + df['sex and age_total population_35 to 44 years']*100\n",
    "bar_4559 = df['sex and age_total population_45 to 54 years']*100 + df['sex and age_total population_55 to 59 years']*100 \n",
    "bar_6075 = df['sex and age_total population_60 to 64 years'] + df['sex and age_total population_65 to 74 years']*100 \n",
    "bar_o75 = df['sex and age_total population_75 to 84 years']*100 + df['sex and age_total population_85 years and over']*100 \n",
    "\n",
    "# cumulative height of bars\n",
    "barx1 = np.add(bar_u25,bar_2544).tolist() # first two bars\n",
    "barx2 = np.add(barx1,bar_4559).tolist() # first three bars\n",
    "barx3 = np.add(barx2,bar_6075).tolist() # first four bars\n",
    "\n",
    "p1 = plt.bar(df['Rate_Per_Thousand'],bar_u25, color='maroon')\n",
    "p2 = plt.bar(df['Rate_Per_Thousand'],bar_2544, bottom=bar_u25, color='salmon')\n",
    "p3 = plt.bar(df['Rate_Per_Thousand'],bar_4559, bottom=barx1, color='lightcoral')\n",
    "p4 = plt.bar(df['Rate_Per_Thousand'],bar_6075, bottom=barx2, color='mistyrose')\n",
    "p5 = plt.bar(df['Rate_Per_Thousand'],bar_o75, bottom=barx3, color='seashell')\n",
    "\n",
    "\n",
    "plt.xlabel(\"# with Diabetes Per Thousand People\")\n",
    "plt.ylabel(\"Age Distribution\")\n",
    "plt.legend((p1[0],p2[0],p3[0],p4[4],p5[0]), (\"<20 yo\", \"25-44 yo\", \"45-59 yo\", \"60-75\", '75+ yo'),loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (iib) Diabetes Rate by Race Breakdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_amin = df['race_total population_one race_american indian and alaska native'] * 100 \n",
    "bar_asian = df['race_total population_one race_asian'] *100\n",
    "bar_blk = df['race_total population_one race_black or african american'] * 100 \n",
    "bar_pacisl = df['race_total population_one race_native hawaiian and other pacific islander'] *100\n",
    "bar_other = df['race_total population_one race_some other race'] *100 + df['race_total population_two or more races'] * 100 \n",
    "bar_white = df['race_total population_one race_white'] *100\n",
    "\n",
    "# cumulative height of bars \n",
    "barx1 = np.add(bar_amin,bar_asian).tolist() \n",
    "barx2 = np.add(barx1,bar_blk).tolist()\n",
    "barx3 = np.add(barx2,bar_pacisl).tolist() \n",
    "barx4 = np.add(barx3,bar_other).tolist() \n",
    "\n",
    "p1 = plt.bar(df['Rate_Per_Thousand'],bar_amin, color='darkblue')\n",
    "p2 = plt.bar(df['Rate_Per_Thousand'],bar_asian, bottom=bar_amin, color='royalblue')\n",
    "p3 = plt.bar(df['Rate_Per_Thousand'],bar_blk, bottom=barx1, color='dodgerblue')\n",
    "p4 = plt.bar(df['Rate_Per_Thousand'],bar_pacisl, bottom=barx2, color='lightskyblue')\n",
    "p5 = plt.bar(df['Rate_Per_Thousand'],bar_other, bottom=barx3, color='powderblue')\n",
    "p6 = plt.bar(df['Rate_Per_Thousand'],bar_white, bottom=barx4, color='lightcyan')\n",
    "\n",
    "\n",
    "plt.xlabel(\"# with Diabetes Per Thousand People\")\n",
    "plt.ylabel(\"Race Distribution\")\n",
    "plt.legend((p1[0],p2[0],p3[0],p4[4],p5[0],p6[0]), (\"Am. Indian\", \"Asian\", \"Black\", \"Pac. Islander\", \"Other\", \"White\"),loc=\"upper left\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT EXPLANATION OF SUBGROUP DISTRIBUTIONS HERE. "
   ]
  },
  {
   "source": [
    "## (iii) Correlation Plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EDA: get correlation with Diabetes_Number\n",
    "topn = 5\n",
    "corr_w_diab_num = df.corr()['Diabetes_Number']\n",
    "cols = corr_w_diab_num.axes[0].to_list()\n",
    "col2corr = {col:corr_w_diab_num[col] for col in cols if col not in ['Rate_Per_Thousand', 'race_total population']}\n",
    "col_corr = sorted(col2corr.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "top_correlated_cols = [p[0] for p in col_corr[:topn]]\n",
    "\n",
    "# add newline in place of underscore for labels\n",
    "df_correlation_plot = df[top_correlated_cols]\n",
    "renamed_col = {col: '\\n'.join(col.split('_')) for col in top_correlated_cols}\n",
    "df_correlation_plot = df_correlation_plot.rename(columns=renamed_col)\n",
    "\n",
    "axes = scatter_matrix(df_correlation_plot, figsize=(30, 30)); # adding a semi colon hides the code - kq\n",
    "for i in range(topn):\n",
    "    for j in range(topn):\n",
    "        # set the format to two decimal numbers\n",
    "        axes[i, j].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "mpl.rcParams['axes.labelsize'] = 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the correlation plot, we plot the correlation between top five features with highest absolute correlation with diabetes rate. Note that all features are now normalized by the population. We found that all of the features have medium to low correlation with diabetes rate: the feature most correlated with diabetes rate is the percentage of African American, with a correlation coefficient of .37. The rest of the features have absolute correlation coefficients lower than .3. The correlation plot also shows that features among the same category, e.g. racial features or population-related features, have high correlations while features from different categories tend to have low correlation. This suggests that when conducting feature selection, we can pick one feature from each category to get maximally uncorrelated set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing to Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature Selection\n",
    "\n",
    "#Convert state to dummy variables\n",
    "onehotencoder = OneHotEncoder()\n",
    "onehotencoder.fit(df['State'].to_numpy().reshape(-1, 1))\n",
    "state_array = onehotencoder.transform(df['State'].to_numpy().reshape(-1, 1)).todense()\n",
    "\n",
    "df_states = pd.DataFrame(state_array)\n",
    "df_wo_states = df.drop(columns=['State'])\n",
    "df_cleaned = pd.concat([df_states, df_wo_states], axis=1)\n",
    "\n",
    "#Drop County and FIPS\n",
    "df_cleaned.drop(columns=['County', 'CountyFIPS'])\n",
    "\n",
    "#Drop rate per thosuand feature created for EDA.\n",
    "df_cleaned = df.drop(columns = \"Rate_Per_Thousand\")"
   ]
  },
  {
   "source": [
    "### 3.3 Feature Selection\n",
    "Note: we do feature selection before partitioning the data.\n",
    "\n",
    "We are interested in only keeping the features that are highly correlated with our target of interest: diabetes rate. Features that are not highly correlated with the target do not provide enough information to improve our prediction, and keeping such features would hinder interpretability and be computationally expensive."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select variable that have correlations above 0.3 with Diabetes_Number\n",
    "cor = df_cleaned.corr()\n",
    "cor_target = abs(cor[\"Diabetes_Number\"])\n",
    "\n",
    "relevant_features = cor_target[cor_target>0.3]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreate data frame, only keeping features that are highly correalted with target\n",
    "\n",
    "df_cleaned = df_cleaned.filter(['Diabetes_Number', 'Obesity_Number', 'Physical_Inactivity_Number',\n",
    "                                        'race_total population_one race_black or african american', \n",
    "                                        'race alone or in combination with one or more other races_total population_black or african american',\n",
    "                                        'hispanic or latino and race_total population_not hispanic or latino_black or african american alone'])\n"
   ]
  },
  {
   "source": [
    "We then computed the linear correlation between the remaining features. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "cor = df_cleaned.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'race_total population_one race_black or african american', 'race alone or in combination with one or more other races_total population_black or african american', and 'hispanic or latino and race_total population_not hispanic or latino_black or african american alone' are perfectly correlated with each other (corr = 1), we select the feature that has the strongest correlation with the target: 'hispanic or latino and race_total population_not hispanic or latino_black or african american' alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(columns = ['race_total population_one race_black or african american',\n",
    "                               'race alone or in combination with one or more other races_total population_black or african american'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert observations to floats\n",
    "for col in df_cleaned.columns:\n",
    "    df_cleaned[col] = df_cleaned[col].astype('float64')\n",
    "\n",
    "print(\"Shape of Cleaned DataFrame:\", df.shape)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Partition Data\n",
    "The percentages of training set, validation set, and test set are 70%, 20%, and 10% respectively. The training set is used to train the models. The validation set is used for tuning hyperparameters and model selection. The test set is used to test the accuracy of the final model. A high training set percentage will lead to lower bias of the models since more data are used in training, but at the same time this will decrease the amount of data for computing accuracy, resulting in higher variance in validation and testing accuracy. We chose a high percentage for the training set since we want to reserve most of the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splittng into features and target\n",
    "features = df_cleaned.drop(columns = [\"Diabetes_Number\"])\n",
    "target = df_cleaned.filter([\"Diabetes_Number\"])\n",
    "\n",
    "## Data partition\n",
    "n_data = df_cleaned.shape[0]\n",
    "n_train = int(n_data*0.7)\n",
    "n_valid = int(n_data*0.2)\n",
    "n_test = n_data - n_train - n_valid\n",
    "\n",
    "labels = np.repeat(['train', 'valid', 'test'], (n_train, n_valid, n_test))\n",
    "np.random.seed(0)\n",
    "randomized_labels = np.random.choice(labels, n_data, replace=False)\n",
    "df_cleaned['group'] = randomized_labels\n",
    "\n",
    "df_train = df_cleaned[df_cleaned['group'] == 'train']\n",
    "df_valid = df_cleaned[df_cleaned['group'] == 'valid']\n",
    "df_test = df_cleaned[df_cleaned['group'] == 'test']\n",
    "\n",
    "df_train = df_train.drop(columns=['group'])\n",
    "df_valid = df_valid.drop(columns=['group'])\n",
    "df_test = df_test.drop(columns=['group'])\n",
    "\n",
    "y_train = df_train['Diabetes_Number']\n",
    "X_train = df_train.drop(columns=['Diabetes_Number'])\n",
    "y_valid = df_valid['Diabetes_Number']\n",
    "X_valid = df_valid.drop(columns=['Diabetes_Number'])\n",
    "X_test = df_test.drop(columns=['Diabetes_Number'])\n",
    "y_test = df_test['Diabetes_Number']\n",
    "\n",
    "#standardize features\n",
    "X_train_scaled = scale(X_train)\n",
    "X_valid_scaled = scale(X_valid)\n",
    "X_test_scaled = scale(X_test)\n",
    "features_scaled = scale(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Models, Predict on Validation Set, Refine Models\n",
    "The five models that we picked are OLS regression, ridge regression, LASSO regression, Elastic-Net, and Random Forest. Hyperparameters are chosen using grid search on the training set except for LASSO, where we had to manually set an extremely small alpha to prevent shrinking all coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) OLS Regression Model\n",
    "OLS model captures the linear relationship between IVs and DV. The strength of OLS is the relative ease of interpretting the coefficients, which quantifies the magnitude and direction of linear effects of IVs on the DV. The drawback of OLS is the lower prediction accuracy compared to other machine learning methods. In addition, if there are highly correlated features in the dataset, the coefficients could be poorly determined, leading to extremely postive or negative coefficients. Interpretation of coefficients could also be a problem if there are a large number of features, since most features will have non-zero coefficients in OLS. Given its simplicity, OLS is a good start for most regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine hyperparameters to use\n",
    "param_grid = {'fit_intercept': ['True', 'False'],\n",
    "              'normalize': ['True', 'False']}\n",
    "\n",
    "lin_grid_reg = GridSearchCV(LinearRegression(), param_grid, cv=3, iid=False)\n",
    "lin_grid_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_index = np.argmax(lin_grid_reg.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "print(lin_grid_reg.cv_results_[\"params\"][best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit linear regression model on  training data using best hyperparameters\n",
    "linear_reg = LinearRegression(**lin_grid_reg.cv_results_[\"params\"][best_index]).fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how well model fits the training data\n",
    "lin_y_hat_train = linear_reg.predict(X_train_scaled)\n",
    "lin_r2_score =  r2_score(y_train, lin_y_hat_train)\n",
    "print(\"R^2 of OLS Training Model:\", lin_r2_score)\n",
    "print(\"Correlation of OLS Training Model:\", math.sqrt(lin_r2_score))\n",
    "\n",
    "sns.scatterplot(y_train, lin_y_hat_train)\n",
    "plt.title('Linear Model (OLS) Predicted v. Actual on Training Data')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the coefficient and feature names\n",
    "lin_reg_data = pd.DataFrame([linear_reg.coef_, X_train.columns]).T\n",
    "lin_reg_data.columns = ['Coefficient', 'Feature']\n",
    "# Plot\n",
    "ax = sns.barplot(x=\"Coefficient\", y=\"Feature\", data=lin_reg_data)\n",
    "ax.set_title(\"OLS Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (ii) Ridge Regression Model\n",
    "Ridge regression also aims to capture linear relations between IVs and the DV, but differ from OLS in that it imposes constraints on the squared sum of coefficients. The constraint is designed to alleviate the wildly varied coefficients when having correlated features in OLS. Similar to OLS, ridge regression also suffers from interpertability issues when having a large number of features. Since from EDA we found a few strongly correlated features in the dataset, ridge regression appears to be an apporiate choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine hyperparameters to use for ridge\n",
    "param_grid = {'alpha': np.arange(.1, 1, .1),\n",
    "               'normalize': ['True', 'False'],\n",
    "             'fit_intercept': ['True', 'False'],\n",
    "             'solver': ['auto', 'svd', 'cholesky', 'lsqr']}\n",
    "\n",
    "ridge_grid_reg = GridSearchCV(Ridge(), param_grid, cv=3, iid=False)\n",
    "ridge_grid_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(ridge_grid_reg.cv_results_[\"params\"][best_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit ridge regression model on  training data using best hyperparameters\n",
    "ridge_reg = Ridge(**ridge_grid_reg.cv_results_[\"params\"][best_index]).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how well model fits the training data\n",
    "ridge_y_hat_train = ridge_reg.predict(X_train_scaled)\n",
    "ridge_r2_score = r2_score(y_train, ridge_y_hat_train)\n",
    "print(\"R^2 of Ridge Training Model:\", ridge_r2_score)\n",
    "print(\"Correlation of Ridge Training Model\", math.sqrt(ridge_r2_score))\n",
    "\n",
    "sns.scatterplot(y_train, ridge_y_hat_train)\n",
    "plt.title('Ridge Predicted v. Actual on Training Data')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the coefficient and feature names\n",
    "ridge_reg_data = pd.DataFrame([ridge_reg.coef_, X_train.columns]).T\n",
    "ridge_reg_data.columns = ['Coefficient', 'Feature']\n",
    "# Plot\n",
    "ax = sns.barplot(x=\"Coefficient\", y=\"Feature\", data=ridge_reg_data)\n",
    "ax.set_title(\"Ridge Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (iii) LASSO Regression Model\n",
    "LASSO is similar to ridge regression in the sense that it also places constraint on the size of the coefficients, but instead of limiting the squared sum of coefficients, LASSO limits the absolute sum of coefficients. Apart from alleviating the issue of wildly varied cofficients of OLS, the absolute sum constraint will shrink some of the coefficients to zero, leaving only the most important features with nonzero coefficients. This improves the interpratbility of coefficients. On the other hand, if the constraint parameter is too large, LASSO could shrink all coefficients to zero, so careful choice of the constraint parameter is necessary. Given the many features in this machine learning problem, LASSO serves as a reasonable model to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Determine hyperparameters to use\n",
    "param_grid = {'normalize': ['True', 'False'],\n",
    "             'fit_intercept': ['True', 'False'],\n",
    "             'selection': ['cyclic', 'random'],\n",
    "             }\n",
    "lasso_grid_reg = GridSearchCV(Lasso(max_iter=10000), param_grid, cv=3, iid=False)\n",
    "lasso_grid_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_index = np.argmax(lasso_grid_reg.cv_results_[\"mean_test_score\"])\n",
    "print(lasso_grid_reg.cv_results_[\"params\"][best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = Lasso(max_iter = 10000, alpha = .00000000001, fit_intercept = True, normalize = True, selection = 'cyclic').fit(X_train_scaled, y_train)\n",
    "#need to hard-code very small alpha so coefficients appear\n",
    "#lasso_reg = Lasso(**lasso_grid_reg.cv_results_[\"params\"][best_index]).fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how well model fits the training data\n",
    "lasso_y_hat_train = lasso_reg.predict(X_train_scaled)\n",
    "lasso_r2_score = r2_score(y_train, lasso_y_hat_train)\n",
    "print(\"R^2 of Lasso Training Model:\", lasso_r2_score)\n",
    "print(\"Correlation of Lasso Training Model\", math.sqrt(lasso_r2_score))\n",
    "\n",
    "sns.scatterplot(y_train, lasso_y_hat_train)\n",
    "plt.title('Lasso Predicted v. Actual on Training Data')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the coefficient and feature names\n",
    "lasso_reg_data = pd.DataFrame([lasso_reg.coef_, X_train.columns]).T\n",
    "lasso_reg_data.columns = ['Coefficient', 'Feature']\n",
    "\n",
    "# Plot\n",
    "ax = sns.barplot(x=\"Coefficient\", y=\"Feature\", data=lasso_reg_data)\n",
    "ax.set_title(\"LASSO Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (iv) Elastic-Net Model\n",
    "## ADD DESCRIPTION OF ELASTIC-NET HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine hyperparameters to use\n",
    "param_grid = {'alpha': np.arange(0.1,1.0,0.1),\n",
    "             'l1_ratio': np.arange(0.0,1.0,.05),\n",
    "             'fit_intercept': ['True', 'False'], \n",
    "             'normalize': ['True', 'False'],\n",
    "             'selection': ['cyclic', 'random']}\n",
    "\n",
    "enet = ElasticNet(max_iter=1000) \n",
    "import warnings # hide convergence warnings \n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "#I think it's would be better to keep the convergence warning, so that we know we have to increase max_iter when the method does not converge - VC\n",
    "# ^ i agree it's safer to include them, but raising max_iter doesn't prevent the warning either and takes a WILD amount of time, so leaving for now until we figure out why - kq \n",
    "enet_grid_reg = GridSearchCV(enet, param_grid, scoring='r2', cv=3, iid=False)\n",
    "\n",
    "enet_grid_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_index = np.argmax(enet_grid_reg.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "print(enet_grid_reg.cv_results_[\"params\"][best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fit enet regression model on  training data using best hyperparameters\n",
    "enet_reg = ElasticNet(**enet_grid_reg.cv_results_[\"params\"][best_index]).fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate how well model fits the training data\n",
    "enet_y_hat_train = enet_reg.predict(X_train_scaled)\n",
    "enet_r2_score = r2_score(y_train, enet_y_hat_train)\n",
    "print(\"R^2 of Elastic Net Training Model:\", enet_r2_score)\n",
    "print(\"Correlation of Elastic Net Training Model\", math.sqrt(enet_r2_score))\n",
    "\n",
    "sns.scatterplot(y_train, enet_y_hat_train)\n",
    "plt.title('Elastic Nete Predicted v. Actual on Training Data')\n",
    "plt.xlabel('actual value')\n",
    "plt.ylabel('predicted value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the coefficient and feature names\n",
    "enet_reg_data = pd.DataFrame([enet_reg.coef_, X_train.columns]).T\n",
    "enet_reg_data.columns = ['Coefficient', 'Feature']\n",
    "\n",
    "# Plot\n",
    "ax = sns.barplot(x=\"Coefficient\", y=\"Feature\", data=enet_reg_data)\n",
    "ax.set_title(\"Elastic Net Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter tuning of the elastic net model yielded a L1 penalty of zero, which means this model is virtually identical to the ridge regression but with different hyperparameters. This is supported by the fact that the coefficients (and, by extension, MSEs) are quite similar in the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (v) Random Forest Model\n",
    "## ADD DESCRIPTION OF RANDOM FOREST HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest - need to tune this a bit more still and use random numbers for the # levels and samples - kq \n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [2,40]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 20]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1,20]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_grid_reg = GridSearchCV(rf,param_grid, cv = 3, n_jobs= -1, verbose = 2)\n",
    "rf_grid_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "best_index = np.argmax(rf_grid_reg.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "print(rf_grid_reg.cv_results_[\"params\"][best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(**rf_grid_reg.cv_results_[\"params\"][best_index]).fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = list(rf_reg.feature_importances_)\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, X_train.columns, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Feature'); plt.title('Feature Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validate and Refine Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Predict on the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fitted each model to the training data, we now predict on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict each fitted model on the validation set\n",
    "\n",
    "linear_y_hat = linear_reg.predict(X_valid_scaled)\n",
    "ridge_y_hat = ridge_reg.predict(X_valid_scaled)\n",
    "lasso_y_hat = lasso_reg.predict(X_valid_scaled)\n",
    "enet_y_hat = enet_reg.predict(X_valid_scaled)\n",
    "rf_y_hat = rf_reg.predict(X_valid_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compare the strength of the models by comparing the mean squared error of each model. We will select the model with the lowest mean squared error and predict that model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualte and compare MSEs \n",
    "\n",
    "mse_linear = mean_squared_error(y_valid, linear_y_hat)\n",
    "mse_ridge = mean_squared_error(y_valid, ridge_y_hat)\n",
    "mse_lasso = mean_squared_error(y_valid, lasso_y_hat) \n",
    "mse_enet = mean_squared_error(y_valid, enet_y_hat)\n",
    "mse_rf = mean_squared_error(y_valid, rf_y_hat)\n",
    "\n",
    "r2_linear = r2_score(y_valid, linear_y_hat)\n",
    "r2_ridge = r2_score(y_valid, ridge_y_hat)\n",
    "r2_lasso = r2_score(y_valid, lasso_y_hat) \n",
    "r2_enet = r2_score(y_valid, enet_y_hat)\n",
    "r2_rf = r2_score(y_valid, rf_y_hat)\n",
    "\n",
    "print('OLS\\tMSE: {}, R^2: {}'.format(mse_linear, r2_linear))\n",
    "print('Ridge\\tMSE: {}, R^2: {}'.format(mse_ridge, r2_ridge))\n",
    "print('LASSO\\tMSE: {}, R^2: {}'.format(mse_lasso, r2_lasso))\n",
    "print('Elastic-Net\\tMSE: {}, R^2: {}'.format(mse_enet, r2_enet)) \n",
    "print('Random Forest\\tMSE: {}, R^2: {}'.format(mse_rf, r2_rf))\n",
    " \n",
    "\n",
    "## @VC and KQ: It looks like OLS is the best?! -LL \n",
    "##Might be worth calculating accuracy scores as well? model.score "
   ]
  },
  {
   "source": [
    "### 5.2 Feature Selection\n",
    "We have already selected features for the test set in section 3.3."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Predicting on Test Set\n",
    "The advantage of using both validation set and test set is that we can avoid overestimating the model accuracy. If only a test set is used, then we might overfit the test set when tuning hyperparameters and selecting models. Given the importance of generalizability of models in social science and public policy, it is crucial to carve out a test set that is used only at the last step to gauge the true accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select best-performing model and predict on the test set.\n",
    "# accoding to the results on the validation set, OLS has the lowest MSE\n",
    "y_hat_best = linear_reg.predict(X_test_scaled)\n",
    "mse_best = mean_squared_error(y_test, y_hat_best)\n",
    "r2_best = r2_score(y_test, y_hat_best)\n",
    "print('MSE best (OLS): ', mse_best)\n",
    "print('R^2 best (OLS): ', r2_best)\n",
    "print('Hyperparameters used')\n",
    "print('--------------------')\n",
    "for param, val in linear_reg.get_params().items():\n",
    "    print('\\t{}: {}'.format(param, val))\n",
    "\n",
    "# OLS actually performs better on the test set haha -VC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Cross Validation\n",
    "Tradeoff with the choice of K: When K is small, each model use only a relatively small fraction of the data, so the estimated error has a larger bias. But since the training sets are more diverse, the estimated error will have smaller variance. When K is large, the bias of estimated error becomes smaller since now we use most of the data for training, but the variance will be larger because the different training sets have considerable overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform 10-fold cross validation\n",
    "#The default score for linear reg is R^2, but since we are using MSE for model selection, I changed it to MSE. -VC\n",
    "mse_scoring = make_scorer(mean_squared_error)\n",
    "scores = cross_val_score(linear_reg, features_scaled, target, cv =10, scoring=mse_scoring)\n",
    "print('Cross-validated means MSE:', np.mean(scores))"
   ]
  },
  {
   "source": [
    "The MSE calculated from cross validation is slight larger than that of using the train/validation/test split (4.5e-4 vs 4.1e-4)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. The bias-variance tradoff and its relevance to ML problems like this one\n",
    "Bias-variance tradeoff is a general phenomenon in machine learning that models which have smaller training errors on average (low bias) will be more sensitive to small perturbations in training samples (high variance). Ideally, we want to find a predictor that fits data well (low bias) consistently across datasets (low variance). So bias-variance tradeoff serves as a reminder that when using more complicated models to achieve better training accuracy, we might be sacrificing generalizability along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Overfitting, why it matters to ML, and ways to address it\n",
    "Overfitting is the situation where a machine learning method has high accuracy in the training set, but perfoms poorly in the test set. It usually happens when people use more complicated models in an attempt to boost prediction accuracy, but the additional flexibility is used to fit noises rather than signals. The problem can be addressed by reserving part of the data as the validation set. First fit different models on the training set, and select models based on performance on the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Discussion\n",
    "From the analysis, we identifed three risk factors of diabetes rate: physical inactivity number, obesity rate, and the ratio of African Americans (NOTE: I'm not sure if it's really the meaning of the third variable. Need to check. -VC), in descending order of importance. The order is consistent across all five models that we used, suggesting that the result is robust to the machine learning methods chosen. All three features are positively correlated with diabetes rate.\n",
    "\n",
    "We recommend prioritizing the counties which (a) already have high diabetes rate or (b) do not currently have high diabetes rate but are so predicted by our OLS regression model. For group (a), the high diabetes rate indicates that there might be systematic factors in the counties that increase the likelihood of developing diabetes, therefore conducting the pilot program there could potentially reduce the future diabetes rate. For group (b), even though the counties do not have high diabetes rate yet, according to our model it is likely that more residents will develop diabetes in the near future. A successful prevention program could prevent that from happening. The results are shown in the following tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 counties with highest diabetes rate\n",
    "df.sort_values('Diabetes_Number', ascending=False)[['County', 'State', 'Diabetes_Number']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 counties with highest predicted diabetes rate\n",
    "topn = 10\n",
    "y_hat_all = linear_reg.predict(features_scaled).tolist()\n",
    "index_ranked = list(range(len(y_hat_all)))\n",
    "# sort county index by predicted y_hat\n",
    "index_yhat = sorted(zip(index_ranked, y_hat_all), key=lambda x: x[1], reverse=True)\n",
    "index_ranked = [p[0] for p in index_yhat]\n",
    "yhat_ranked = np.array([p[1] for p in index_yhat])\n",
    "# get county by index\n",
    "df_highest_yhat = df.loc[index_ranked[:topn]][['County', 'State', 'Diabetes_Number']]\n",
    "df_highest_yhat['Predicted Diabetes Number'] = yhat_ranked[:topn]\n",
    "df_highest_yhat"
   ]
  },
  {
   "source": [
    "We feel pretty confident (or do we? haha -VC) about deploying this model in real-world applications. It is not only because the model performs pretty well on the test set which is untouched throughout the training process, but also because the relative importance of features are consistent across the five models we have tried. More generally, the approach taken in this project allows us to systematically test the suitability of different machine learning models. If the selected model performs well on the test set, we can be assured about its generalizability outside the training data. Even if all of the models fail to provide satisfactory results, we can safely conclude that the problem at hand is beyond the reach of the models we have tried. Overall, we feel comfortable about applying machine learning methods to our fields of study."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}